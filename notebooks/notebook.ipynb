{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amiru Chandrasena & Kelvin Young"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Cells Bassed On Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cells Dataset contains two components, one component is the CSV document containing the features about the cell and the cells corresponding image url. Secondly there is also a folder of images on cells in PNG format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1] Reading In The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of data analysis is reading in the data from the CSV file, secondly the next step is exploring the data to look for errors and tredns in the dataset that may help with optimising the approach and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "dataSet_cells = pd.read_csv('data/data_labels_mainData.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.1] Exploring The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the data involves firstly checking the head and the tail of the data to ensure that the CSV file has been compleately read in without errors. This includes checking the first couple of records, the last records and the features. Secondly the next step is to check for missing values to ensure that the model is not introduced to incomplete data and interfearing with the performance of the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet_cells.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet_cells.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet_cells.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = dataSet_cells.isnull().sum()\n",
    "\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet_cells.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet_cells.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.2] Stripping Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is importnat to drop unimportant fatures from the dataset for example 'InstanceID', 'patientID' ..etc are irrelevant to the training of the model. THe only important information that is used if the numeric column called 'cellType' that assigns a numeric value to the 'cellTypeName' attributes and the 'ImageName'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Drop the specified features\n",
    "dataSet_cells.drop([\"InstanceID\", \"patientID\", \"cellTypeName\", \"isCancerous\"], axis=1, inplace=True)\n",
    "dataSet_cells.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.3] Splitting The Dataset Into Training, Validaiton and Testing Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into the three section allows for the effecive testing of the dataset. Allocating 80%(5937) of the dataset to the the training of the model allows for th emodel to have an increased accuracy, while allowin for the validation set to test the accuracy of the model without exposing it to the final test data leaving it unseen. The validation set allows for the base models accuracy to be gauged allowing infromation about the model to be taken such as if its overfitted or underfitted and what possible hyperparameters need to be changed. \n",
    "\n",
    "We then use a histogram to check the distribution of train set attributes is approximately equal to the distribution of test set attributes. This is important for the following reasons:\n",
    "\n",
    "Generalisation: By ensuring that the data is a fair sample of the the whole dataset for bot hthe validaiton and the test set, you increase the likelihood that the model will be able to generalise well to unseen data. If the test set has a significantly different distribution, the model may struggle to make accurate predictions because it hasn't learned the relevant patterns or variations during training. Further there can be bias as the data in the validation set may skew one way and not be representing of the whole dataset. \n",
    "\n",
    "Ultimately this helps in building a robust and reliable model that can generalize well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the data into 80% of training Data, 10% Validation and 10% Testing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_data, test_data = train_test_split(dataSet_cells, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.25, random_state=42)\n",
    "\n",
    "print(\"Train data : {}, Val Data: {}, Test Data: {}\".format(train_data.shape, val_data.shape, test_data.shape))\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot histogram for train set\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(train_data['cellType'], bins=range(4), alpha=0.5, label='Train')\n",
    "plt.hist(val_data['cellType'], bins=range(4), alpha=0.5, label='Validation')\n",
    "plt.hist(test_data['cellType'], bins=range(4), alpha=0.5, label='Test')\n",
    "plt.xlabel('cellType')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of cellType in Train, Validation, and Test sets')\n",
    "plt.xticks(range(4))\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above shows the spread of data and we see that the validation adn test set are very similar and representative of the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [2] Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2.1] Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base model begins with defining the dimensions of the image with (16*16*3) turinnig this into an array, secondly the hidden layer defines 256 neurons to start which will be tunned later to ensure that the model is working effectively. Thirdly as there are 4 different types of cells there will be 4 different output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = (16,16,3)\n",
    "HIDDEN_LAYER_DIM = 256\n",
    "OUTPUT_CLASSES = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2.2] Create a sequential API to build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three different types of models in tensorflow and the Sequential model was used for this NN as it is simplistic in the manner it works allowsing for models to be built layer by layer and the simplicity for it to be trained. \n",
    "\n",
    "The three layers were used in the model as first the image is flattend as mentioned in the previous cell, there will be no learning done in the layer and only image processing. THe second layer is responsible for the learning as it uses a sigmoid acitvation. THen the final layer is responcible for the output of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=INPUT_DIM), ## Flatten the images into (16x16x3) array\n",
    "    tf.keras.layers.Dense(HIDDEN_LAYER_DIM, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(OUTPUT_CLASSES)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2.3] Compiling The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the feature title to a string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['cellType'] = train_data['cellType'].astype('str')\n",
    "val_data['cellType'] = val_data['cellType'].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2.4] Image Generation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Systematically load in the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing: Preprocess your images as needed to make them suitable for training with a feedforward neural network. This may involve resizing the images to a fixed size and converting them to a suitable format (e.g., grayscale or RGB channels).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, data_format='channels_last')\n",
    "val_datagen = ImageDataGenerator(rescale=1./255, data_format='channels_last')\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_data,\n",
    "        directory='patch_images',\n",
    "        x_col=\"ImageName\",\n",
    "        y_col=\"cellType\",\n",
    "        target_size=(16, 16),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_dataframe(\n",
    "        dataframe=val_data,\n",
    "        directory='patch_images',\n",
    "        x_col=\"ImageName\",\n",
    "        y_col=\"cellType\",\n",
    "        target_size=(16, 16),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2.5] Fitting & Training The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_generator, validation_data=validation_generator, epochs=50, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_learning_curve(train_loss, val_loss, train_acc, val_acc, metric_name='Accuracy'):\n",
    "    # Plotting the Loss Curve\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_loss, 'r-', label='Training Loss')\n",
    "    plt.plot(val_loss, 'b-', label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plotting the Accuracy Curve\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_acc, 'r-', label='Training ' + metric_name)\n",
    "    plt.plot(val_acc, 'b-', label='Validation ' + metric_name)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.title('Accuracy Curve')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot the learning curves\n",
    "plot_learning_curve(history.history['loss'], history.history['val_loss'], \n",
    "                    history.history['categorical_accuracy'], history.history['val_categorical_accuracy'], \n",
    "                    metric_name='Accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, the spiky behavior of the validation loss suggests that the model is not generalizing well to unseen data. It indicates that the model's performance on the validation set is inconsistent and fluctuates significantly during training. This could be a sign of overfitting, where the model is learning to fit the training data too closely and struggles to generalize to new examples\n",
    "\n",
    "The spikier and increasing trend of the training accuracy compared to the first graph could be an indication that the model is memorizing the training data, leading to high accuracy on the training set but poor generalization to new examples.\n",
    "\n",
    "If the model is over fitting:\n",
    "Regularization (Lasso/Ridge penalty): Regularization techniques like L1 (Lasso) and L2 (Ridge) regularization can help control the complexity of the model by adding a penalty term to the loss function. This penalty discourages large weights and encourages the model to learn simpler and more generalizable representations. You can experiment with different regularization strengths to find the right balance between reducing overfitting and preserving model performance.\n",
    "\n",
    "Dropout: Dropout is a regularization technique where randomly selected neurons are temporarily \"dropped out\" during training. This helps prevent the model from relying too heavily on specific neurons and encourages the network to learn more robust and generalized features. Dropout introduces some level of randomness and acts as an ensemble of multiple sub-networks during training, reducing overfitting.\n",
    "\n",
    "Reduce the number of neurons or layers: Simplifying the model architecture by reducing the number of neurons or layers can help reduce overfitting. A complex model with a large number of parameters is more prone to overfitting, especially if the training data is limited. By reducing the model's capacity, you can encourage it to learn more generalized patterns and reduce the risk of overfitting.\n",
    "\n",
    "plus use scaling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuned Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "651c3b7b3f466eb030aa737f279c846be4db9cba49a2f229278cab5e41121ed5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
